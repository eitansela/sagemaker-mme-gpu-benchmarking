{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c5a31fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Benchmark autoscaling RoBERTa base model using Amazon SageMaker Multi-model endpoints (MME) with GPU support\n",
    "\n",
    "Amazon SageMaker multi-model endpoints with GPU works using NVIDIA Triton Inference Server. NVIDIA Triton Inference Server is open-source inference serving software that simplifies the inference serving process and provides high inference performance. Triton supports all major training and inference frameworks, such as TensorFlow, NVIDIA TensorRT, PyTorch, MXNet, Python, ONNX, XGBoost, scikit-learn, RandomForest, OpenVINO, custom C++, and more. It offers dynamic batching, concurrent execution, post-training quantization, optimal model configuration to achieve high performance inference.\n",
    "\n",
    "In this notebook, we are going to run benchmark testing for the most popluar NLP models using MME on GPU. We will evaluate model performance such as the inference latency, throughput, and optimum model count per instance. We will also compile these models using NVIDA TensorRT to compare performance against TorchScript models.\n",
    "\n",
    "This notebook is tested on `PyTorch 1.12 Python 3.8 CPU Optimized` kernel on SageMaker Studio. An instance with at least 8 vCPU cores such as an `ml.c5.2xlarge` is recommended to run the load test. A smaller instance may be utilized by reducing the scale of the load test. The configuration provide here simulates up to 200 concurrent workers      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b237a7c",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "\n",
    "Installs the dependencies required to package the model and run inferences using Triton server.\n",
    "\n",
    "Also define the IAM role that will give SageMaker access to the model artifacts and the NVIDIA Triton ECR image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1552a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install timm -Uqq\n",
    "%pip install transformers -Uqq\n",
    "%pip install locust -Uqq\n",
    "%pip install boto3 -Uqq\n",
    "%pip install sagemaker -Uqq\n",
    "%pip install matplotlib -Uqq\n",
    "%pip install Jinja2 -Uqq\n",
    "%pip install ipywidgets -Uqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e659fb-6fe6-4cbd-8f45-890e531cce46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)  # has to restart kernel so changes are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec1001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%env TOKENIZERS_PARALLELISM=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8b4725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import datetime as dt\n",
    "import warnings\n",
    "\n",
    "from utils import model_utils\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "account = sess.account_id()\n",
    "bucket = sess.default_bucket() # or use your own custom bucket name\n",
    "prefix = 'mme-roberta-base-benchmark'\n",
    "\n",
    "use_case = \"nlp\"\n",
    "\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "tested_models = [\"roberta-base\"]\n",
    "\n",
    "model_name = \"roberta-base\" #change the model name to benchmark different NLP models\n",
    "\n",
    "max_seq_len = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5503b2",
   "metadata": {},
   "source": [
    "Account Id Mapping for triton inference containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1041ca81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abb439f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise(\"UNSUPPORTED REGION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d4db3",
   "metadata": {},
   "source": [
    "## Generate Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fed9ec",
   "metadata": {},
   "source": [
    "We are going to use the following SageMaker Processing script to generate our pretrained model. This script does the following:\n",
    "\n",
    "1. Generate a model using the Pytorch Hub\n",
    "\n",
    "2. jit script the model and save the torchscript file\n",
    "\n",
    "3. Create a model artifact which is comprised of the torchscript file and a model configuration (config.pbtxt) for Triton serving\n",
    "\n",
    "Helper functions have been created for each of these steps and are imported from the `utils.model_utils` local module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f0bad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if model_name in tested_models:\n",
    "    tokenizer, model = model_utils.get_model_from_hf_hub(model_name)\n",
    "else:\n",
    "    warnings.warn(f\"{model_name} has not been tested and may not work\")\n",
    "    tokenizer, model = model_utils.get_model_from_hf_hub(model_name)\n",
    "model.eval()\n",
    "\n",
    "print(f\"loaded model {model_name} with {model_utils.count_parameters(model)} parameters\")\n",
    "\n",
    "example_input = tokenizer(\"This is a sample\", padding=\"max_length\", max_length=max_seq_len, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976213f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Packaging Pytorch model for Triton sever on SageMaker\n",
    "\n",
    "**Note**: SageMaker expects the model tarball file to have a top level directory with the same name as the model defined in the `config.pbtxt`.\n",
    "\n",
    "```\n",
    "model_name\n",
    "├── 1\n",
    "│   └── model.pt\n",
    "└── config.pbtxt\n",
    "```\n",
    "\n",
    "We will be tracing an existing RoBERTa base model for the purpose of converting PyTorch modules to TorchScript - PyTorch high-performance deployment runtime. \n",
    "\n",
    "[This tutorial](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html) is an introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b6fba1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pytorch_model_path = Path(f\"triton-serve-pt/{model_name}/1\")\n",
    "pytorch_model_path.mkdir(parents=True, exist_ok=True)\n",
    "pt_model_path = model_utils.export_pt_jit(model, list(example_input.values()), pytorch_model_path) #export jit compiled model to specified directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8dae48",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> <strong> Note: </strong>\n",
    "Based on the architecture of the model we will generate a Triton configuration (config.pbtxt) file. This approach should work for most models but you may need to make adjustments to the generated config. Additionally a base model is assumed that will return the output from the last hidden state. If using a different output head such as a sequence classification, adjust the triton_outputs variable below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eff00a-c60e-4165-9b63-91a2cda829c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get input names \n",
    "triton_inputs = [\n",
    "    {\"name\": input_name, \"data_type\": \"TYPE_INT32\", \"dims\": f\"[{max_seq_len}]\"}\n",
    "    for input_name in example_input\n",
    "]\n",
    "triton_outputs = [\n",
    "    {\n",
    "        \"name\": \"last_hidden_state\",\n",
    "        \"data_type\": \"TYPE_FP32\",\n",
    "        \"dims\": f\"[{max_seq_len}, {model.config.hidden_size}]\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec8066a-b2a2-451c-914f-d7740d789623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "triton_config_path = model_utils.generate_triton_config(platform=\"pt\", triton_inputs=triton_inputs,  triton_outputs=triton_outputs, save_path=pytorch_model_path)\n",
    "triton_config_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e9f4f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We'll package a model config template along with the compiled model into a model.tar.gz artifact. \n",
    "# The config templates assume batch size of 32 and sequence length of 128\n",
    "# You may need to adjust the template if not using one of the tested models\n",
    "model_atifact_path = model_utils.package_triton_model(model_name, pt_model_path, triton_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf85a69c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mme_path = f\"s3://{bucket}/{prefix}/{model_name}/\"\n",
    "initial_model_path = sess.upload_data(model_atifact_path.as_posix(), bucket=bucket, key_prefix=f\"{prefix}{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe70241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c65b114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mme_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c13fc9",
   "metadata": {},
   "source": [
    "We make sure there are no models located in the Multi Model Endpoint path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f2a19-2c50-4d2a-abc2-e58c50b76041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! aws s3 rm --recursive {mme_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f2d4df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls {mme_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859b150",
   "metadata": {},
   "source": [
    "## Create a SageMaker Multi-Model Endpoint for PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b99513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.endpoint_utils import create_endpoint, delete_endpoint, get_instance_utilization, run_autoscaling_load_test\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "mme_triton_image_uri = f\"{account_id_map[region]}.dkr.ecr.{region}.{base}\" + \\\n",
    "            \"/sagemaker-tritonserver:22.10-py3\"\n",
    "print(mme_triton_image_uri)\n",
    "instance_type = 'ml.g4dn.xlarge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248712a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "container = {\n",
    "    \"Image\": mme_triton_image_uri,\n",
    "    \"ModelDataUrl\": mme_path,\n",
    "    \"Mode\": \"MultiModel\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4875fa1e",
   "metadata": {},
   "source": [
    "We'll deploy and endpoint is deployed using a helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb3dbc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_model_name, endpoint_config_name, endpoint_name = create_endpoint(sm_client, model_name, role, container, instance_type, \"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ccd9b7",
   "metadata": {},
   "source": [
    "Next we'll upload a python model that we can use to query the instance utilization in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291029f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar czvf metrics.tar.gz server_metrics/\n",
    "!aws s3 cp metrics.tar.gz {mme_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c5e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls {mme_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb93dab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_instance_utilization(runtime_sm_client, endpoint_name) #invoke once to load the python model in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6999a89",
   "metadata": {},
   "source": [
    "## Load PyTorch Models into Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0214e9cb-a908-42fb-8d3f-655b2b5a7989",
   "metadata": {},
   "source": [
    "In this section we will invoke the Endpoint to make sure it is working and returning predictions and then load 100 models into the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db320eee-4f60-485a-b17c-c6a0e2c4be7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\":\n",
    "        [{\"name\": name, \"shape\": list(data.size()), \"datatype\": \"INT32\", \"data\": data.tolist()} for name, data in example_input.items()]\n",
    "}\n",
    "payload['inputs'][0]['shape']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba411381-5eb9-420e-8407-c1492a800476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 cp roberta-base.tar.gz {mme_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b7beba-bbcc-4348-a42e-96f196c4b711",
   "metadata": {},
   "source": [
    "#### Invoke the Endpoint to make sure it is working and returning predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0a1e17-8ddb-425c-8695-0caf0f8cc54a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "targetModel=\"roberta-base.tar.gz\"\n",
    "print(f\"invoking endpoint with traget model: {targetModel}\")\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/octet-stream\",\n",
    "            Body=json.dumps(payload),\n",
    "            TargetModel=targetModel, \n",
    "        )\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5bb1be-2f69-46eb-979c-bc88a7d4818a",
   "metadata": {},
   "source": [
    "#### Load 100 models into the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7365832-4136-4caf-bcf3-c1b718a5974d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_models_to_load = 100\n",
    "models_loaded = 0\n",
    "while models_loaded < max_models_to_load:\n",
    "    !aws s3 cp {initial_model_path} {mme_path}{model_name}-v{models_loaded}.tar.gz\n",
    "    models_loaded = models_loaded+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730509c7-68fd-43ec-9be9-5fa4971e64ee",
   "metadata": {},
   "source": [
    "#### Invoke the Endpoint to make sure it is working with a copied model and returning predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcba746a-7a56-46e4-a205-2f2e2c12d0f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "targetModel=f\"{model_name}-v0.tar.gz\"\n",
    "print(f\"invoking endpoint with traget model: {targetModel}\")\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/octet-stream\",\n",
    "            Body=json.dumps(payload),\n",
    "            TargetModel=targetModel, \n",
    "        )\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b341933-d503-4262-816a-7f7257f32df4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls {mme_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823ebd12-db0a-453e-8b30-d26ab5d1532f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d653ec02-6ce5-4807-be89-cbbf8d4073da",
   "metadata": {},
   "source": [
    "## Add Auto Scaling policy\n",
    "\n",
    "Currently we are setting a very low threshold of no of invocations per instance as 1 so it will start the scale up almost right away. For production loads this needs to be tuned and set appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b04eaa-e347-44cb-bb6a-6c17e3eff764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaling_client = boto3.client(\n",
    "    \"application-autoscaling\"\n",
    ")  # Common class representing Application Auto Scaling for SageMaker amongst other services\n",
    "\n",
    "resource_id = (\n",
    "    \"endpoint/\" + endpoint_name + \"/variant/\" + \"AllTraffic\" #\"variant1\"\n",
    ")  # This is the format in which application autoscaling references the endpoint\n",
    "print(resource_id)\n",
    "\n",
    "# Configure Autoscaling on asynchronous endpoint down to zero instances\n",
    "response = scaling_client.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=4,\n",
    ")\n",
    "\n",
    "response = scaling_client.put_scaling_policy(\n",
    "    PolicyName=\"Invocations-ScalingPolicy\",\n",
    "    ServiceNamespace=\"sagemaker\",  # The namespace of the AWS service that provides the resource.\n",
    "    ResourceId=resource_id,  # Endpoint name\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",  # SageMaker supports only Instance Count\n",
    "    PolicyType=\"TargetTrackingScaling\",  # 'StepScaling'|'TargetTrackingScaling'\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"TargetValue\": 1.0, #0.5, #30,  # 1 or 70 -- > based on your workload\n",
    "        \"PredefinedMetricSpecification\": {\n",
    "            \"PredefinedMetricType\": \"SageMakerVariantInvocationsPerInstance\", # is the average number of times per minute that each instance for a variant is invoked. \n",
    "        },\n",
    "        \"ScaleInCooldown\": 600,  # The cooldown period helps you prevent your Auto Scaling group from launching or terminating\n",
    "        # additional instances before the effects of previous activities are visible.\n",
    "        # You can configure the length of time based on your instance startup time or other application needs.\n",
    "        # ScaleInCooldown - The amount of time, in seconds, after a scale in activity completes before another scale in activity can start.\n",
    "        \"ScaleOutCooldown\": 5  # ScaleOutCooldown - The amount of time, in seconds, after a scale out activity completes before another scale out activity can start.\n",
    "        # 'DisableScaleIn': True|False - ndicates whether scale in by the target tracking policy is disabled.\n",
    "        # If the value is true , scale in is disabled and the target tracking policy won't remove capacity from the scalable resource.\n",
    "    },\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c48f44-9f0b-4682-ab8f-538eb58c6078",
   "metadata": {},
   "source": [
    "## Benchmark Pytorch Model using Locust with autoscaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eabe995-400a-4447-ae73-2d19357b36ee",
   "metadata": {},
   "source": [
    "`locust_benchmark_sm.py` is provided in the 'locust' folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39668aca-ad28-4a57-8709-a21a753861e3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> <strong> Note: </strong>\n",
    "The load test is run with up to 20 simulated workers. This may not be suitable for larger models with long response times. You can modify the <code>StagesShape</code> Class in the <code>locust/locust_benchmark_sm.py</code> file to adjust the traffic pattern and the number of concurrent workers\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d708def2-0be1-448c-aa9f-4ceef33c8f6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "locust_result_path = Path(\"results\") / model_name / \"autoscaling\"\n",
    "locust_result_path.mkdir(parents=True,exist_ok=True)\n",
    "locust_result_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c2aa7c-bc35-4cfb-a737-c31cc556b054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "output_path = (locust_result_path / f\"{instance_type}*pt*{models_loaded}\") # capture the instance type, engine, and models loaded in file name\n",
    "run_autoscaling_load_test(endpoint_name, use_case, model_name, models_loaded, output_path, print_stdout=True, n_procs=6, sample_payload=json.dumps(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e1e70-4ccb-4a28-8001-85d9678d7e06",
   "metadata": {},
   "source": [
    "## Plot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767eb078-048a-4b99-a6c0-29fc3232312a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cw = boto3.Session().client(\"cloudwatch\")\n",
    "\n",
    "def get_sagemaker_utilization_metrics(\n",
    "    endpoint_name,\n",
    "    endpoint_config_name,\n",
    "    variant_name,\n",
    "    metric_name,\n",
    "    statistic,\n",
    "    start_time,\n",
    "    end_time,\n",
    "    period=1\n",
    "):\n",
    "    dimensions = [\n",
    "        {\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "        {\"Name\": \"VariantName\", \"Value\": variant_name},\n",
    "    ]\n",
    "    if endpoint_config_name is not None:\n",
    "        dimensions.append({\"Name\": \"EndpointConfigName\", \"Value\": endpoint_config_name})\n",
    "        \n",
    "    if metric_name in [\"CPUUtilization\", \"MemoryUtilization\", \"DiskUtilization\", \"LoadedModelCount\",]:\n",
    "        namespace = \"/aws/sagemaker/Endpoints\"\n",
    "    else:\n",
    "        namespace = \"AWS/SageMaker\"\n",
    "    metrics = cw.get_metric_statistics(\n",
    "        Namespace=namespace, #\"aws/sagemaker/Endpoints\",\n",
    "        MetricName=metric_name,\n",
    "        StartTime=start_time,\n",
    "        EndTime=end_time,\n",
    "        Period=period, #1, #period, #60,  # 1,#60,\n",
    "        Statistics=[statistic],\n",
    "        Dimensions=dimensions,\n",
    "    )\n",
    "    rename = endpoint_config_name if endpoint_config_name is not None else \"ALL\"\n",
    "    #print(metrics)\n",
    "    df = pd.DataFrame(metrics[\"Datapoints\"])\n",
    "    if df.empty:\n",
    "        print(f\"EmptyDF:UTIL:: metric_name={metric_name}::statistic={statistic}::period={period}::namespace={namespace}::\")\n",
    "        return df\n",
    "\n",
    "    return (\n",
    "        df.sort_values(\"Timestamp\")\n",
    "        .set_index(\"Timestamp\")\n",
    "        .drop([\"Unit\"], axis=1)\n",
    "        .rename(columns={statistic: rename})\n",
    "    )\n",
    "\n",
    "\n",
    "def get_sagemaker_metrics(\n",
    "    endpoint_name,\n",
    "    endpoint_config_name,\n",
    "    variant_name,\n",
    "    metric_name,\n",
    "    statistic,\n",
    "    start_time,\n",
    "    end_time,\n",
    "):\n",
    "    dimensions = [\n",
    "        {\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "        {\"Name\": \"VariantName\", \"Value\": variant_name},\n",
    "    ]\n",
    "    if endpoint_config_name is not None:\n",
    "        dimensions.append({\"Name\": \"EndpointConfigName\", \"Value\": endpoint_config_name})\n",
    "        \n",
    "    if metric_name in [\"CPUUtilization\", \"MemoryUtilization\", \"DiskUtilization\"]:\n",
    "        namespace = \"/aws/sagemaker/Endpoints\"\n",
    "    else:\n",
    "        namespace = \"AWS/SageMaker\"\n",
    "\n",
    "    print(f\"Metrics:namespace={namespace}::metric_name={metric_name}::statistic={statistic}:dimensions={dimensions}:endpoint_name={endpoint_name}:\")\n",
    "    metrics = cw.get_metric_statistics(\n",
    "        Namespace=namespace,\n",
    "        MetricName=metric_name,\n",
    "        StartTime=start_time,\n",
    "        EndTime=end_time,\n",
    "        Period=60, # 1,#60,\n",
    "        Statistics=[statistic,],\n",
    "        Dimensions=dimensions,\n",
    "    )\n",
    "    rename = endpoint_config_name if endpoint_config_name is not None else \"ALL\"\n",
    "    # print(metrics)\n",
    "    df = pd.DataFrame(metrics[\"Datapoints\"])\n",
    "    if df.empty:\n",
    "        print(f\"EmptyDF:CUST: metric_name={metric_name}::statistic={statistic}::\")\n",
    "        return df\n",
    "\n",
    "    return (\n",
    "        df.sort_values(\"Timestamp\")\n",
    "        .set_index(\"Timestamp\")\n",
    "        .drop([\"Unit\"], axis=1)\n",
    "        .rename(columns={statistic: rename})\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_endpoint_model_latency_metrics(\n",
    "    endpoint_name,\n",
    "    endpoint_config_name,\n",
    "    variant_name,\n",
    "    start_time=None,\n",
    "    end_time=datetime.now(),\n",
    "    metric_name=\"ModelLatency\",\n",
    "    statistic=\"Average\",\n",
    "):\n",
    "    start_time = start_time or datetime.now() - timedelta(minutes=60)\n",
    "    # end_time = datetime.now()\n",
    "    # metric_name = \"ModelLatency\"\n",
    "    # statistic = \"Average\"\n",
    "    metrics_variants = get_sagemaker_metrics(\n",
    "        endpoint_name,\n",
    "        endpoint_config_name,\n",
    "        variant_name,\n",
    "        metric_name,\n",
    "        statistic,\n",
    "        start_time,\n",
    "        end_time,\n",
    "    )\n",
    "    if metrics_variants.empty:\n",
    "        print(\n",
    "            f\"NO RESULTS for metric_name={metric_name}::statistic={statistic}::start_time={start_time}:: end_time={end_time}:: endpoint_name={endpoint_name}: \"\n",
    "        )\n",
    "        return\n",
    "    metrics_variants.plot(title=f\"{metric_name}-{statistic}\")\n",
    "    return metrics_variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3889243f-c61c-45a1-92a9-d3e6228dc2ca",
   "metadata": {},
   "source": [
    "### Get total invocations per minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2dcffc-d525-4f1b-9a3e-cce5ebc201f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = datetime.now() - timedelta(minutes=22)  # - 60\n",
    "end_time = datetime.now()# - timedelta(minutes=30)  # - 30, 660, datetime.now() #-- minutes=30\n",
    "metric_name = \"Invocations\" #\"Invocations\", #\"InvocationsPerInstance\"  # \"ModelLatency\"\n",
    "statistics = \"Sum\" #\"Sum\"  # \"Maximum\" #\"Average\"\n",
    "invocations_metrics = plot_endpoint_model_latency_metrics(\n",
    "    endpoint_name, None, \"AllTraffic\", start_time, end_time, metric_name, statistics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da4e60-777d-4d39-aa07-d2c0d6430d15",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get invocations per instance, per minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e47f30-5966-41a8-8f19-f860fcb79b2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = datetime.now() - timedelta(minutes=22)  # - 60\n",
    "end_time = datetime.now()# - timedelta(minutes=30)  # - 30, 660, datetime.now() #-- minutes=30\n",
    "metric_name = \"InvocationsPerInstance\" #\"Invocations\", #\"InvocationsPerInstance\"  # \"ModelLatency\"\n",
    "statistics = \"Sum\" #\"Sum\"  # \"Maximum\" #\"Average\"\n",
    "invocations_per_instance_metrics = plot_endpoint_model_latency_metrics(\n",
    "    endpoint_name, None, \"AllTraffic\", start_time, end_time, metric_name, statistics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eb73f4-2e17-4eaf-be14-862ebcbfb54f",
   "metadata": {},
   "source": [
    "### Get number of instances at any point of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992fd9fc-6a7e-41c3-8d09-c3d8159d65f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = datetime.now() - timedelta(minutes=22)  # - 60\n",
    "end_time = datetime.now()# - timedelta(minutes=30)  # - 30, 660, datetime.now() #-- minutes=30\n",
    "metric_name = \"CPUUtilization\" #\"InvocationsPerInstance\"  # \"ModelLatency\"\n",
    "statistic = \"SampleCount\" #\"Sum\"  # \"Maximum\" #\"Average\"\n",
    "# -- SampleCount, Average, Sum, Minimum, Maximum]\n",
    "insatcnes_count = get_sagemaker_utilization_metrics(\n",
    "        endpoint_name=endpoint_name,\n",
    "        endpoint_config_name=None,\n",
    "        variant_name=\"AllTraffic\",\n",
    "        metric_name=metric_name,\n",
    "        statistic=statistic,\n",
    "        start_time=start_time,\n",
    "        end_time=end_time,\n",
    "        period=1\n",
    "    )\n",
    "insatcnes_count[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81c6f5-2fbe-4d32-b299-c5dbd24c7f50",
   "metadata": {},
   "source": [
    "### Get loaded model count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30261e0f-bfb7-47bf-bb8d-1be075108b58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = datetime.now() - timedelta(minutes=22)  # - 60\n",
    "end_time = datetime.now()# - timedelta(minutes=30)  # - 30, 660, datetime.now() #-- minutes=30\n",
    "metric_name = \"LoadedModelCount\" #\"InvocationsPerInstance\"  # \"ModelLatency\"\n",
    "statistic = \"Sum\" #\"SampleCount\" #\"Sum\"  # \"Maximum\" #\"Average\"\n",
    "# -- SampleCount, Average, Sum, Minimum, Maximum]\n",
    "loaded_model_count = get_sagemaker_utilization_metrics(\n",
    "        endpoint_name=endpoint_name,\n",
    "        endpoint_config_name=None,\n",
    "        variant_name=\"AllTraffic\",\n",
    "        metric_name=metric_name,\n",
    "        statistic=statistic,\n",
    "        start_time=start_time,\n",
    "        end_time=end_time,\n",
    "        period=1\n",
    "    )\n",
    "loaded_model_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43334963-b643-495f-99bb-47ece6fca602",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Clean up auto scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cc5ad6-6994-4632-888f-89c94a6198be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaling_client = boto3.client(\n",
    "    \"application-autoscaling\"\n",
    ")  # Common class representing Application Auto Scaling for SageMaker amongst other services\n",
    "\n",
    "response = scaling_client.deregister_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f71c8-089e-4da1-b3ce-255fd7981024",
   "metadata": {},
   "source": [
    "## Clean Up PyTorch Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85195772",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_endpoint(sm_client, sm_model_name, endpoint_config_name, endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0351464-a625-45f7-b7b9-56d75253cb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 rm --recursive {mme_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ef1f65-e193-48f2-bdc2-63ecf2da4291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.c5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.12 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.12-cpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd01d8a33ae048de921784525e60c8784d22ac368cf7370d33c1ec56f2410197"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
